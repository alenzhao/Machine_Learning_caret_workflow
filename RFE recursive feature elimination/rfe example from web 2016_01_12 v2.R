# Recursive Feature Elimination using the rfe function in caret
#  From http://topepo.github.io/caret/rfe.html

# clear the decks.
rm(list=ls(all=TRUE))

library(caret)
library(mlbench)
library(Hmisc)
library(randomForest)

# To test the algorithm, the "Friedman 1" benchmark (Friedman, 1991) was used. 
# There are five informative variables generated by the equation (EQUATION NOT GIVEN HERE, SEE WEB SITE)

n <- 100
p <- 40
sigma <- 1
set.seed(1)
sim <- mlbench.friedman1(n, sd = sigma)
colnames(sim$x) <- c(paste("real", 1:5, sep = ""),paste("bogus", 1:5, sep = ""))

bogus <- matrix(rnorm(n * p), nrow = n)   
colnames(bogus) <- paste("bogus", 5+(1:ncol(bogus)), sep = "")
x <- cbind(sim$x, bogus)
y <- sim$y

# Of the 50 predictors, there are 45 pure noise variables: 5 are uniform on [0, 1] and 40 are random univariate standard normals. 
# The predictors are centered and scaled:

normalization <- preProcess(x)
x <- predict(normalization, x)
x <- as.data.frame(x)
subsets <- c(1:5, 10, 15, 20, 25)


# The simulation will fit models with subset sizes of 25, 20, 15, 10, 5, 4, 3, 2, 1.
# 
# As previously mentioned, to fit linear models, the lmFuncs set of functions can be used. 
# To do this, a control object is created with the rfeControl function. 
# We also specify that repeated 10-fold cross-validation should be used in line 2.1 of Algorithm 2. 
# The number of folds can be changed via the number argument to rfeControl (defaults to 10). 
# The verbose option prevents copious amounts of output from being produced.

set.seed(10)

ctrl <- rfeControl(functions = lmFuncs,
                   method = "repeatedcv",
                   repeats = 5,
                   verbose = FALSE)

lmProfile <- rfe(x, y,
                 sizes = subsets,
                 rfeControl = ctrl)

lmProfile

# The output shows that the best subset size was estimated to be 4 predictors. 
# This set includes informative variables but did not include them all. 
# The predictors function can be used to get a text string of variable names that were picked in the final model. 
# The lmProfile is a list of class "rfe" that contains an object fit that is the final linear model with the remaining terms. 
# The model can be used to get predictions for future or test samples.

predictors(lmProfile)
lmProfile$fit
head(lmProfile$resample)

# There are also several plot methods to visualize the results. plot(lmProfile) 
   # produces the performance profile across different subset sizes, as shown in the figure below.

trellis.par.set(caretTheme())
plot(lmProfile, type = c("g", "o"))

# Also the resampling results are stored in the sub-object lmProfile$resample and can be used with several lattice functions. 
# Univariate lattice functions (densityplot, histogram) can be used to plot the resampling distribution 
#   while bivariate functions (xyplot, stripplot) can be used to plot the distributions for different subset sizes. 
# In the latter case, the option returnResamp = "all" in rfeControl can be used to save all the resampling results. 
# Example images are shown below for the random forest model.
