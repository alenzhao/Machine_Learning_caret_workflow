#                order="hclust", hclust.method="ward")
# clear the decks.
rm(list=ls(all=TRUE))
DATE <- "2016_01_08"
GOOGLE_DRIVE <- "F:\\Google Drive\\"     # home
# GOOGLE_DRIVE <- "C:\\Google Drive\\"     # work
programStartTime <- Sys.time()
set.seed(123)
# NOTE - this package list is not comprehensive, you will have to install other packages depending on
#          which ML methods you choose to run
require(caret)        # Main Machine Learning package
require(ggplot2)      # Plotting
require(corrplot)     # for examination of variable correlations
require(klaR)         # Miscellaneous functions for classification and visualization developed
#at the Fakultaet Statistik, Technische Universitaet Dortmund
require(MASS)         # Functions and datasets to support Venables and Ripley, "Modern Applied Statistics with S"
# (4th edition, 2002).
require(e1071)        # Functions for latent class analysis, short time Fourier transform,
# fuzzy clustering, support vector machines, shortest path computation, bagged clustering,
# naive Bayes classifier, ...
require(DMwR)         # This package includes functions and data accompanying the book "Data Mining with R,
# learning with case studies" by Luis Torgo, CRC Press 2010.
require(nnet)         # Neural Net
# Input file has all chemoinfo data removed.
path <- paste0(GOOGLE_DRIVE, "Toxcast\\00 Shah Paper\\Data\\")
filename <- "tx500501h_si_002 ORIGINAL REMOVE CHEMOINFO.csv"
temp <- read.csv(paste0(path, filename), stringsAsFactors=FALSE)
# cgange row names to CASRN numbers
row.names(temp) <- temp$CASRN
# remove some cols
temp1 <- subset(temp, select = -c(Chemical_name, CASRN, CODE, Hypertrophy, Proliferative.lesions, Negative))
# rename col "Injury" to "Class"
# What we are trying to predict has to be named "Class" for this code to work
# change Class to factor and replace 0 with "non-toxic" and 1 with "toxic"
colnames(temp1)[colnames(temp1)=="Injury"] <- "Class"
as.character(temp1$Class)
temp1$Class[temp1$Class == 0] <- "nontoxic"
temp1$Class[temp1$Class == 1] <- "heptoxic"
temp1$Class <- as.factor(temp1$Class)
# check for NAs which can be a big hassle in some ML programs
if(anyNA(temp1)) {
print("There are NAs in this dataset")
rowHasNA<- apply(temp1, 1, function(x){any(is.na(x))})
rowNumberNA <- which(rowHasNA)
paste(print("Row(s) with NA:"))
print(rowNumberNA)
rowContents <- temp1[rowNumberNA ,]
colHasNA <- which(is.na(rowContents))
print("Column(s) with NA:")
print(colHasNA)
}  else {
print("No NAs in this dataset")
}
# Check for variables with near zero variance
# Data needs to be ALL NUMERIC, adjust for your data by removing character variables
# Remvove Class column
temp2 <- subset(temp1, select = -c(Class))
# nearZeroVar returns the number of near zero variance cols
# These variables should be considered for elimination.
nzv <- nearZeroVar(temp2)
if( length(nzv) == 0) {nzv <- 0}
print(paste("Number of columns with near zero variance is: ", nzv), quote=FALSE)
# Look at variable distribution in each Class
# This code will probably only work for Categorical predictions.
featurePlot(temp2, dataRaw$Class, "strip")
# Examine highly correlated variables for possible exclusion using hierarchical clustering.
# Again, data needs to be all numeric.
# Nice example of corrplot.mixed at  https://rpubs.com/flyingdisc/practical-machine-learning-xgboost
# It's pretty but I'm not using it :)
# Can export plot as PNG or PDF
# mar - numerical vector indicating margin size c(bottom, left, top, right) in lines.
# NEEDS IMPROVEMENT - long axis labels can get chopped off
# corrplot.mixed(cor(temp), lower="circle", upper="number",
#                mar = c(1, 4, 4, 1),
#                tl.cex=0.3,     # size of labels
#                cex.main=0.5,   # title text size
#                tl.pos="lt",
#                diag="n",
#                title=paste("Correlations of", length(names(temp)), "variables in the dataset"),
#                order="hclust", hclust.method="ward")
corrplot(cor(temp2),
order="hclust",
hclust.method="ward",
mar = c(1, 3, 4, 1),
title=paste("Correlations of", length(names(temp)), "variables in dataset"),
tl.cex = 0.3,     # labels text size
cex.main=1         # title text size
)
# Again looking for variables to exclude
# Change cutoff as you see fit
# From documentation:
#   The absolute values of pair-wise correlations are considered. If two variables have a high correlation, the
#     function looks at the mean absolute correlation of each variable and removes the variable with the largest
#     mean absolute correlation.
# Using exact = TRUE will cause the function to re-evaluate the average correlations
# at each step while exact = FALSE uses all the correlations regardless of whether
# they have been eliminated or not. The exact calculations will remove a smaller
# number of predictors but can be much slower when the problem dimensions are "big".
highlyCorrelated <- findCorrelation(cor(temp2), cutoff=0.95, exact=TRUE)
print(paste("Highly correlated variables for consideration for elmination:",
names(temp)[highlyCorrelated]) )
# No variables removed.  Try and keep things consistent with Shah analysis.
# IMPORTANT - dataframe for next code segment must be named "dataPreProc"
dataPreProc <- temp1
# Look at data
names(temp1)
dim(temp1)
table(temp1$Class)
sessionInfo()
packageVersion("caret")
featurePlot(temp2, temp1$Class, "strip")
nzv <- nearZeroVar(temp2)
if( length(nzv) == 0) {nzv <- 0}
print(paste("Number of columns with near zero variance is: ", nzv), quote=FALSE)
corrplot(cor(temp2),
order="hclust",
hclust.method="ward",
mar = c(1, 3, 4, 1),
title=paste("Correlations of", length(names(temp)), "variables in dataset"),
tl.cex = 0.3,     # labels text size
cex.main=1         # title text size
)
corrplot(cor(temp2),
order="hclust",
hclust.method="ward",
mar = c(1, 3, 4, 1),
title=paste("Correlations of", length(names(temp2)), "variables in dataset"),
tl.cex = 0.3,     # labels text size
cex.main=1         # title text size
)
highlyCorrelated <- findCorrelation(cor(temp2), cutoff=0.95, exact=TRUE)
print(paste("Highly correlated variables for consideration for elmination:",
names(temp2)[highlyCorrelated]) )
# clear the decks.
rm(list=ls(all=TRUE))
DATE <- "2016_01_08"
GOOGLE_DRIVE <- "F:\\Google Drive\\"     # home
# GOOGLE_DRIVE <- "C:\\Google Drive\\"     # work
programStartTime <- Sys.time()
set.seed(123)
# NOTE - this package list is not comprehensive, you will have to install other packages depending on
#          which ML methods you choose to run
require(caret)        # Main Machine Learning package
require(ggplot2)      # Plotting
require(corrplot)     # for examination of variable correlations
require(klaR)         # Miscellaneous functions for classification and visualization developed
#at the Fakultaet Statistik, Technische Universitaet Dortmund
require(MASS)         # Functions and datasets to support Venables and Ripley, "Modern Applied Statistics with S"
# (4th edition, 2002).
require(e1071)        # Functions for latent class analysis, short time Fourier transform,
# fuzzy clustering, support vector machines, shortest path computation, bagged clustering,
# naive Bayes classifier, ...
require(DMwR)         # This package includes functions and data accompanying the book "Data Mining with R,
# learning with case studies" by Luis Torgo, CRC Press 2010.
require(nnet)         # Neural Net
# Input file has all chemoinfo data removed.
path <- paste0(GOOGLE_DRIVE, "Toxcast\\00 Shah Paper\\Data\\")
filename <- "tx500501h_si_002 ORIGINAL REMOVE CHEMOINFO.csv"
temp <- read.csv(paste0(path, filename), stringsAsFactors=FALSE)
# cgange row names to CASRN numbers
row.names(temp) <- temp$CASRN
# remove some cols
temp1 <- subset(temp, select = -c(Chemical_name, CASRN, CODE, Hypertrophy, Proliferative.lesions, Negative))
# rename col "Injury" to "Class"
# What we are trying to predict has to be named "Class" for this code to work
# change Class to factor and replace 0 with "non-toxic" and 1 with "toxic"
colnames(temp1)[colnames(temp1)=="Injury"] <- "Class"
as.character(temp1$Class)
temp1$Class[temp1$Class == 0] <- "nontoxic"
temp1$Class[temp1$Class == 1] <- "heptoxic"
temp1$Class <- as.factor(temp1$Class)
# check for NAs which can be a big hassle in some ML programs
if(anyNA(temp1)) {
print("There are NAs in this dataset")
rowHasNA<- apply(temp1, 1, function(x){any(is.na(x))})
rowNumberNA <- which(rowHasNA)
paste(print("Row(s) with NA:"))
print(rowNumberNA)
rowContents <- temp1[rowNumberNA ,]
colHasNA <- which(is.na(rowContents))
print("Column(s) with NA:")
print(colHasNA)
}  else {
print("No NAs in this dataset")
}
# Check for variables with near zero variance
# Data needs to be ALL NUMERIC, so remove Class column for this.
# nearZeroVar returns the number of near zero variance cols
# These variables should be considered for elimination.
temp2 <- subset(temp1, select = -c(Class))
nzv <- nearZeroVar(temp2)
if( length(nzv) == 0) {nzv <- 0}
print(paste("Number of columns with near zero variance is: ", nzv), quote=FALSE)
# Examine highly correlated variables for possible exclusion.
# Again, data needs to be all numeric.
corrplot(cor(temp2),
order="hclust",
hclust.method="ward",
mar = c(1, 3, 4, 1),
title=paste("Correlations of", length(names(temp2)), "variables in dataset"),
tl.cex = 0.3,     # labels text size
cex.main=1         # title text size
)
# Again looking for variables to exclude
# From caret documentation:
#   The absolute values of pair-wise correlations are considered. If two variables have a high correlation, the
#     function looks at the mean absolute correlation of each variable and removes the variable with the largest
#     mean absolute correlation.
# Using exact = TRUE will cause the function to re-evaluate the average correlations
# at each step while exact = FALSE uses all the correlations regardless of whether
# they have been eliminated or not. The exact calculations will remove a smaller
# number of predictors but can be much slower when the problem dimensions are "big".
highlyCorrelated <- findCorrelation(cor(temp2), cutoff=0.95, exact=TRUE)
print(paste("Highly correlated variables for consideration for elmination:",
names(temp2)[highlyCorrelated]) )
# No variables removed.  Try and keep things consistent with Shah analysis.
# IMPORTANT - dataframe for next code segment must be named "dataPreProc"
dataPreProc <- temp1
# Look at data
names(temp1)
dim(temp1)
table(temp1$Class)
sessionInfo()
packageVersion("caret")
setwd("F:/Google Drive/R/caret/Select Most Dissimilar Models")
rm(list=ls(all=TRUE))
setwd("F:/Google Drive/R/caret/Select Most Dissimilar Models")   # home
# setwd("C:/Google Drive/R/caret/Select Most Dissimilar Models")   # work
tag <- read.csv("tag_data.csv", row.names = 1)
tag <- as.matrix(tag)
## Select only models for Regression or Classification
regModels <- tag[tag[,"Regression"] == 1,]
all <- 1:nrow(regModels)
## Seed the analysis with the SVM model
start <- grep("(svmRadial)", rownames(regModels), fixed = TRUE)
pool <- all[all != start]
## Select 4 model models by maximizing the Jaccard
## dissimilarity between sets of models
nextMods <- maxDissim(regModels[start,,drop = FALSE],
regModels[pool, ],
method = "Jaccard",
n = 4)
rownames(regModels)[c(start, nextMods)]
regModels <- tag[tag[,"Classification"] == 1,]
all <- 1:nrow(regModels)
## Seed the analysis with the SVM model
start <- grep("(svmRadial)", rownames(regModels), fixed = TRUE)
pool <- all[all != start]
## Select 4 model models by maximizing the Jaccard
## dissimilarity between sets of models
nextMods <- maxDissim(regModels[start,,drop = FALSE],
regModels[pool, ],
method = "Jaccard",
n = 4)
rownames(regModels)[c(start, nextMods)]
# Select Most Dissimlar Models For Evaluation Using Caret
# Neal Cariello
# 8 Jan 2016
# clear the decks.
rm(list=ls(all=TRUE))
setwd("F:/Google Drive/R/caret/Select Most Dissimilar Models")   # home
# setwd("C:/Google Drive/R/caret/Select Most Dissimilar Models")   # work
tag <- read.csv("tag_data.csv", row.names = 1)
tag <- as.matrix(tag)
## Select only models for Regression or Classification
regModels <- tag[tag[,"Classification"] == 1,]
all <- 1:nrow(regModels)
## Seed the analysis with the SVM model
start <- grep("(svmRadial)", rownames(regModels), fixed = TRUE)
pool <- all[all != start]
## Select 4 model models by maximizing the Jaccard
## dissimilarity between sets of models
nextMods <- maxDissim(regModels[start,,drop = FALSE],
regModels[pool, ],
method = "Jaccard",
n = 6)
rownames(regModels)[c(start, nextMods)]
